{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21654e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivampr21/codegen/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.http.exceptions import UnexpectedResponse\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4214e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastEmbedEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcee8eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Configure paths and settings\n",
    "REPO_PATH = \"./\"  # Current repository path\n",
    "COLLECTION_NAME = \"code-repository\"\n",
    "OLLAMA_MODEL = \"gemma3:12b\"  # You can change to any model available in your Ollama instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f70dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivampr21/codegen/.venv/lib/python3.13/site-packages/langchain_community/embeddings/fastembed.py:109: UserWarning: The model thenlper/gte-large now uses mean pooling instead of CLS embedding. In order to preserve the previous behaviour, consider either pinning fastembed version to 0.5.1 or using `add_custom_model` functionality.\n",
      "  values[\"model\"] = fastembed.TextEmbedding(\n",
      "Fetching 5 files: 100%|██████████| 5/5 [03:31<00:00, 42.38s/it] \n"
     ]
    }
   ],
   "source": [
    "# Step 2: Set up the embedding model\n",
    "embeddings = FastEmbedEmbeddings(model_name=\"thenlper/gte-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f35de915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize Qdrant client\n",
    "client = QdrantClient(path=\"./qdrant_db\")  # Store locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034e24ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"S\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4591b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3120019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'code-repository' already exists\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create or get collection\n",
    "try:\n",
    "    collection_info = client.get_collection(collection_name=COLLECTION_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' already exists\")\n",
    "except (UnexpectedResponse, ValueError):\n",
    "    # Create new collection with cosine similarity\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=(len(embeddings.embed_query(\"Nishtha Pandey\"))), distance=Distance.COSINE),\n",
    "    )\n",
    "    print(f\"Created new collection: '{COLLECTION_NAME}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a4d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eca745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Load and process code repository\n",
    "def load_code_repository(repo_path):\n",
    "    # Define file extensions to include\n",
    "    code_extensions = [\".py\", \".js\", \".java\", \".cpp\", \".c\", \".h\", \".cs\", \".go\", \".rb\", \".php\", \".ts\", \".html\", \".css\"]\n",
    "\n",
    "    # Create a list of glob patterns for each extension\n",
    "    glob_patterns = [f\"**/*{ext}\" for ext in code_extensions]\n",
    "\n",
    "    # Load all code files\n",
    "    all_files = []\n",
    "    for pattern in glob_patterns:\n",
    "        loader = DirectoryLoader(\n",
    "            repo_path,\n",
    "            glob=pattern,\n",
    "            loader_cls=TextLoader,\n",
    "            show_progress=True\n",
    "        )\n",
    "        files = loader.load()\n",
    "        all_files.extend(files)\n",
    "\n",
    "    print(f\"Loaded {len(all_files)} code files from repository\")\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38806b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Split documents into chunks\n",
    "def process_documents(documents):\n",
    "    # Use a code-optimized splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=150,\n",
    "        separators=[\"\\nclass \", \"\\ndef \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Created {len(chunks)} chunks from code repository\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ffe94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Index the repository\n",
    "def index_repository():\n",
    "    documents = load_code_repository(REPO_PATH)\n",
    "    chunks = process_documents(documents)\n",
    "\n",
    "    # Add documents to vector store\n",
    "    vector_store.add_documents(documents=chunks)\n",
    "    print(\"Repository indexed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96a466fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Set up the LLM with Ollama\n",
    "llm = OllamaLLM(model=OLLAMA_MODEL, base_url=\"http://100.98.113.21:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4219387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Create a custom prompt template for code-related queries\n",
    "prompt_template = \"\"\"\n",
    "You are an expert software developer assistant. Use the following pieces of context from the code repository to answer the question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e0c5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Create the RAG chain\n",
    "def create_rag_chain():\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5}  # Retrieve top 5 most relevant chunks\n",
    "    )\n",
    "\n",
    "    # Create the RAG chain\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f52ab65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Query function\n",
    "def query_code_repository(query, rag_chain=None):\n",
    "    if rag_chain is None:\n",
    "        rag_chain = create_rag_chain()\n",
    "\n",
    "    result = rag_chain({\"query\": query})\n",
    "\n",
    "    return {\n",
    "        \"answer\": result[\"result\"],\n",
    "        \"source_documents\": result.get(\"source_documents\", [])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "937dc0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Index the repository (only need to run this once or when code changes)\n",
    "#     index_repository()\n",
    "\n",
    "#     # Create the RAG chain\n",
    "#     rag_chain = create_rag_chain()\n",
    "\n",
    "#     # Example usage\n",
    "#     # while True:\n",
    "#     query = input(\"\\nEnter your code question (or 'exit' to quit): \")\n",
    "#     # if query.lower() == 'exit':\n",
    "#     #     break\n",
    "\n",
    "#     result = query_code_repository(query, rag_chain)\n",
    "\n",
    "#     print(\"\\nAnswer:\")\n",
    "#     print(result[\"answer\"])\n",
    "\n",
    "#     print(\"\\nSources:\")\n",
    "#     for i, doc in enumerate(result[\"source_documents\"]):\n",
    "#         print(f\"\\nSource {i+1}:\")\n",
    "#         print(f\"Path: {doc.metadata.get('source', 'Unknown')}\")\n",
    "#         print(f\"Content: {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0064a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "# from langchain_community.llms import Ollama\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "# Reuse the RAG chain from the previous code\n",
    "rag_chain = create_rag_chain()\n",
    "\n",
    "def process_query(query):\n",
    "    if not query.strip():\n",
    "        return \"Please enter a valid query.\"\n",
    "\n",
    "    result = query_code_repository(query, rag_chain)\n",
    "\n",
    "    # Format the response\n",
    "    response = result[\"answer\"]\n",
    "\n",
    "    # Add source information\n",
    "    response += \"\\n\\n**Sources:**\\n\"\n",
    "    for i, doc in enumerate(result[\"source_documents\"]):\n",
    "        source_path = doc.metadata.get('source', 'Unknown')\n",
    "        response += f\"\\n{i+1}. {source_path}\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# Create the Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=process_query,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question about your code repository...\"),\n",
    "    outputs=\"markdown\",\n",
    "    title=\"Local Code Repository RAG System\",\n",
    "    description=\"Ask questions about your code repository and get context-aware answers.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d2847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
